{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NeuroData (http://neurodata.io)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "from .base import BaseClassify\n",
    "\n",
    "\n",
    "class SparseOpt(BaseClassify): #check baseclassify\n",
    "    \"\"\"\n",
    "    Network classification algorithm using sparse optimization.\n",
    "    Fits a regularized logistic regression to a set of network adjacency matrices with responses, and returns an\n",
    "    object with the classifier.The classifier fits a matrix of coefficients.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        A matrix with the training samples, in which each row represents the vectorized (by column order) upper\n",
    "        triangular part of a network adjacency matrix.\n",
    "    y : ndarray, [0, 1]\n",
    "        A vector containing the class labels of the training samples.\n",
    "    xtest : ndarray\n",
    "        Matrix containing the test samples, with each row representing an upper-triangular vectorized adjacency\n",
    "        matrix.\n",
    "    ytest : ndarray\n",
    "        The labels of `xtest`.\n",
    "    lambda_ : float, optional\n",
    "        Penalty parameter, by default is 0.\n",
    "    rho : float, optional\n",
    "        Penalty parameter controlling sparsity, by default is 0.\n",
    "    gamma : float, optional\n",
    "        Ridge parameter (for numerical purposes), by default is 1e-5.\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        Containing the trained graph classifier:\n",
    "            - beta : ndarray\n",
    "                Edge coefficients vector of the regularized logistic regression solution.\n",
    "            - b : float\n",
    "                Intercept value\n",
    "            - yfit : ndarray\n",
    "                Fitted logistic regression probabilities in the train data.\n",
    "            - ypred : ndarray\n",
    "                Predicted class for the test samples\n",
    "            - train_error : float\n",
    "                Percentage of train samples that are misclassified.\n",
    "            - test_error : float\n",
    "                Percentage of test samples that are misclassified.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Relion, D. Kessler, E. Levina, S. Taylor.  \"Network Classification\n",
    "        with applications to brain connectomics,\" arXiv: 1701.08140 [stat.ME], 2019\n",
    "    \"\"\"\n",
    "    #Check with TAs about if this satisfies graspy api, what is necessary for users\n",
    "    def __init__(self, lambda_=0, gamma=1e-5, nodes=None):\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.nodes = nodes\n",
    "    \n",
    "    def _admm(self, omega1, omega2, tol=1e-7, beta_start=None):\n",
    "        n = self.y.size\n",
    "        m = self.D.shape[0]\n",
    "\n",
    "        if not beta_start:\n",
    "            beta = self.y\n",
    "        else:\n",
    "            beta = beta_start\n",
    "\n",
    "        soft_beta = _soft_threshold(self.y, omega1)\n",
    "        q = soft_beta\n",
    "        r = self.D @ q\n",
    "        if np.max(np.absolute(q)) == 0:\n",
    "            beta = q\n",
    "            i = 0\n",
    "            conv_crit = 0\n",
    "            best_beta = q\n",
    "\n",
    "            self.beta = beta\n",
    "            self.q = q\n",
    "            self.r = r\n",
    "            self.iter = i\n",
    "            self.conv_crit = conv_crit\n",
    "            self.best_beta = best_beta\n",
    "\n",
    "            return beta, q, r, i, conv_crit, best_beta\n",
    "\n",
    "        qk = q\n",
    "        rk = r\n",
    "        u = np.zeros(n)\n",
    "        v = np.zeros(m)\n",
    "        i = 1\n",
    "        phi_beta_k = (0.5*np.sum(self.y-beta) + omega1*np.sum(np.abs(beta)) \n",
    "                      + omega2*_gl_penalty(self.D @ beta))\n",
    "        conv_crit = np.infty\n",
    "        sk = np.infty\n",
    "        resk = np.infty\n",
    "        best_beta = beta\n",
    "        best_phi = phi_beta_k\n",
    "\n",
    "        while (resk > tol or sk > tol) and i <= self.max_iter:\n",
    "            aux = self.y-u + self.rho*q - self.D @ (self.rho*r - v)\n",
    "            beta = aux / (1+3*self.rho)\n",
    "            Dbeta = self.D @ beta\n",
    "\n",
    "            # update q\n",
    "            q = _soft_threshold(beta+u/self.rho, omega1/self.rho)\n",
    "\n",
    "            # update r\n",
    "            Dbetavrho = Dbeta + v/self.rho\n",
    "            r = _soft_threshold(Dbetavrho, omega2/self.rho)\n",
    "\n",
    "            u = u + self.rho * (beta-q)\n",
    "            v = v + self.rho * (Dbeta-r)\n",
    "\n",
    "            # update convergence criteria\n",
    "            phi_beta_k1 = (0.5*np.sum(self.y-beta)\n",
    "                           + omega1*np.sum(np.abs(beta))\n",
    "                           + omega2*_gl_penalty(self.D, self.nodes))\n",
    "            sk = (self.rho * np.max(np.absolute(q-qk))\n",
    "                  + np.max(np.absolute(np.cross(self.D, r-rk))))\n",
    "\n",
    "            res1k = np.sqrt(np.sum(beta-q))\n",
    "            res2k = np.sqrt(np.sum(Dbeta-r))\n",
    "\n",
    "            resk = res1k + res2k\n",
    "            qk = q\n",
    "            rk = r\n",
    "            conv_crit = np.abs(phi_beta_k1 - phi_beta_k) / phi_beta_k\n",
    "            phi_beta_k = phi_beta_k1\n",
    "\n",
    "            if phi_beta_k1 < best_phi:\n",
    "                best_beta = beta\n",
    "                best_phi = phi_beta_k\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        beta_q = beta * (q == 0).astype(int)\n",
    "        phi_beta_q = (0.5*np.sum(self.y-beta_q)\n",
    "                      + omega1*np.sum(np.abs(beta_q))\n",
    "                      + omega2*_gl_penalty(self.D @ beta_q))\n",
    "        whichm = np.min([phi_beta_k1, best_phi, phi_beta_q])\n",
    "        if whichm == 1:\n",
    "            best_beta = beta\n",
    "        elif whichm == 3:\n",
    "            best_beta = beta_q\n",
    "\n",
    "        self.beta = beta\n",
    "        self.q = q\n",
    "        self.r = r\n",
    "        self.iter = i\n",
    "        self.conv_crit = conv_crit\n",
    "        self.best_beta = best_beta\n",
    "\n",
    "        return beta, q, r, i, conv_crit, best_beta\n",
    "\n",
    "    def _logistic_lasso(self, lambda1, lambda2):\n",
    "        self.rho = 1\n",
    "        n = self.y.size\n",
    "        p = self.x.shape[1]\n",
    "        b_derivative = lambda xbeta, b: (np.sum(-self.y / (1 +\n",
    "            np.exp(self.y * (xbeta+b)))) / n)\n",
    "        b_hessian = lambda xbeta, b: (np.sum(1 / (np.exp(-self.y * (xbeta+b))\n",
    "            + np.exp(self.y * (xbeta+b)) + 2)) / n)\n",
    "\n",
    "        grad_f = lambda xbeta, b, beta: (-np.cross(self.x,\n",
    "            self.y / (1 + np.exp(self.y * (xbeta+b)))) / n\n",
    "            + self.gamma*self.beta)\n",
    "        f = lambda xbeta, b, beta: (np.sum(np.log(1 + np.exp(-self.y\n",
    "            * (xbeta+b)))) / n + self.gamma/2 * np.cross(beta, beta))\n",
    "        penalty = lambda beta: (lambda1*np.sum(np.abs(beta))\n",
    "            + lambda2*_gl_penalty(self.D@beta, self.nodes))\n",
    "\n",
    "        def proximal(u, lambda_, beta_startprox=None, tol=1e-7):\n",
    "            if lambda2 > 0:\n",
    "                gl = self._admm(omega1=lambda_*lambda1,\n",
    "                    omega2=lambda_*lambda2, beta_start=beta_startprox)\n",
    "                return gl[5], gl[1], gl[2]\n",
    "            elif lambda1 > 0:\n",
    "                return np.sign(np.max(np.abs(u) - (lambda1*lambda_), 0))\n",
    "            else:\n",
    "                return u\n",
    "\n",
    "        def b_step(xbeta, b_start=0):\n",
    "            tolb = 1e-4\n",
    "            max_sb = 100\n",
    "            b_n = b_start\n",
    "            i = 0\n",
    "            b_deriv = np.inf\n",
    "            while np.abs(b_derivative) > tolb and i < max_sb:\n",
    "                b_deriv = b_deriv(xbeta, b_n)\n",
    "                b_hess = b_hessian(xbeta, b_n)\n",
    "                b_n = b_deriv/(b_hess + 1 * (np.abs(b_deriv/b_hess) > 100))\n",
    "                i += 1\n",
    "            return b_n\n",
    "\n",
    "        beta_start = np.zeros(p)\n",
    "        b_start = 0\n",
    "\n",
    "        optimal = self._fista(proximal, b_step, f, grad_f, penalty,\n",
    "                              beta_start, b_start)\n",
    "\n",
    "        return optimal\n",
    "\n",
    "    def _fista(self, proximal, b_step, f, grad_f, penalty, beta_start,\n",
    "               b_start): #Finish up this\n",
    "        xk1 = self.x\n",
    "        xk = self.x\n",
    "        criterion = np.infty\n",
    "        crit_f = np.infty\n",
    "\n",
    "        k = 0\n",
    "        tk = 0.125\n",
    "        beta_step = 0.5\n",
    "\n",
    "        xbeta = x @ x_start\n",
    "        b = b_step(xbeta, b_start)\n",
    "        best_beta = self.x\n",
    "        best_b = b\n",
    "        \n",
    "        best_f = f(xbeta, b, x_start) + penalty(x_start, b)\n",
    "        \n",
    "        newf = best_f\n",
    "        \n",
    "        optimal = [xk1, xk, criterion, crit_f, k, tk, beta_step, b]\n",
    "        \n",
    "        return optimal\n",
    "\n",
    "    def _predict(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, y, xtest):      \n",
    "        self.nodes = (1 + np.sqrt(1 + 8*x.shape[1])) / 2\n",
    "        yorig = y\n",
    "        self.y_pos_label = \n",
    "        self.y_neg_label = \n",
    "        self.y = 2 * (y - np.min(y)) / (np.max(y) - np.min(y)) - 1\n",
    "\n",
    "        alpha_norm = np.std(x)\n",
    "        self.x = x / alpha_norm\n",
    "        lambda1 = self.lambda_ * self.rho / alpha_norm\n",
    "        lambda2 = self.lambda_ / alpha_norm\n",
    "        gamma = self.gamma\n",
    "        self.D = _construct_d(self.nodes)\n",
    "\n",
    "        self.beta_start = np.zeros(self.nodes*(self.nodes-1)/2)\n",
    "        self.b_start = 0\n",
    "        self.max_iter = 200\n",
    "        self.conv_crit = 1e-5\n",
    "        self.max_time = np.infty\n",
    "\n",
    "        self._logistic_lasso(lambda1, lambda2)\n",
    "        self.beta = self.best_beta / alpha_norm\n",
    "        self.b = self.best_b \n",
    "        yfit = alpha_norm * (self.x @ beta)\n",
    "        self.yfit = np.exp(yfit) / (1 + np.exp(yfit))\n",
    "        self.train_error = 1 - np.sum(np.diag(yfit)) / len(self.y)\n",
    "        self._predict()\n",
    "        yfit_test = xtest @ beta + b\n",
    "        #Added Ypred: \n",
    "        self.ypred = np.exp(ypred) / (1 + np.exp(ypred))\n",
    "        #Add active_nodes to actually make this a subgraph thing? Compare with vertex screening accuracy\n",
    "        self.yfit_test = np.exp(yfit_test) / (1 + np.exp(yfit_test))\n",
    "        self.test_error = 1 - np.sum(np.diag(yfit_test)) / len(yfit_test)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "@njit\n",
    "def _soft_threshold(x, lambda_, nodes=None):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        norm_node = _gl_penalty(x, nodes)\n",
    "        t = 1 - lambda_/norm_node\n",
    "        for j in range((nodes-1)*i, (nodes-1)*(i+1)):\n",
    "            if norm_node <= lambda_:\n",
    "                x[j] = 0\n",
    "            else:\n",
    "                x[j] *= t\n",
    "\n",
    "    return x\n",
    "\n",
    "@njit\n",
    "def _gl_penalty(b, nodes=None):\n",
    "    for i in range(nodes):\n",
    "        norm_node = np.sum([b[j] ** 2 for j in range((nodes-1)*i, (nodes-1)*(i+1))])\n",
    "        gl = np.sqrt(norm_node)\n",
    "\n",
    "    return gl\n",
    "\n",
    "\n",
    "@njit\n",
    "def _construct_d(nodes=264):\n",
    "    B = np.zeros(shape=(nodes, nodes))\n",
    "    B[np.triu_indices(nodes, k=1)] = [i * (i+1) / 2 for i in range(nodes)]\n",
    "    B = B + B.T\n",
    "    D = np.zeros(shape=(nodes*(nodes-1), nodes*(nodes-1)/2))\n",
    "    for i in range(nodes):\n",
    "        D[i*(nodes-1) + 1:(i+1)*(nodes-1) + 1, B[i, -i]] = np.diag(nodes-1)\n",
    "\n",
    "    return D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
