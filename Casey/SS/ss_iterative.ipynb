{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspy\n",
    "from mgcpy.independence_tests.mgc import MGC\n",
    "from mgcpy.independence_tests.dcorr import DCorr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "from graspy.simulations import sbm\n",
    "from graspy.plot import heatmap\n",
    "np.random.seed(10)\n",
    "\n",
    "#Generating simulation/test data\n",
    "def data_generator(m_n_0, ss_m_n, p1, p2, p3, p4, p5, num_graphs):\n",
    "    data = {}\n",
    "    male = {}\n",
    "    female = {}\n",
    "    ind_male = 0\n",
    "    ind_female = 0\n",
    "    #This creates dictionaries of test data\n",
    "    for i in range(0,num_graphs):\n",
    "        n = [ss_m_n, m_n_0 - ss_m_n]\n",
    "        if i % 2 == 0:\n",
    "            p_male = [[p1, p2], [p3, p4]]\n",
    "            male[\"male{0}\".format(ind_male)] = sbm(n=n, p=p_male)\n",
    "            data[\"male{0}\".format(i)] = sbm(n=n, p=p_male)\n",
    "            ind_male += 1\n",
    "        else:\n",
    "            p_female = [[p5, p2], [p3, p4]]\n",
    "            female[\"female{0}\".format(ind_female)] = sbm(n=n, p=p_female)\n",
    "            data[\"female{0}\".format(i)] = sbm(n=n, p=p_female)\n",
    "            ind_female += 1\n",
    "    return male,female,data\n",
    "\n",
    "male, female, data = data_generator(200, 20, .30, .2, .2, .3, .45, 100)\n",
    "\n",
    "#Plot\n",
    "heatmap(female[\"female1\"], title='Female Graph Simulation')\n",
    "\n",
    "plt.figure()\n",
    "heatmap(male[\"male1\"], title='Male Graph Simulation')\n",
    "\n",
    "#Uses L_class_tool on all entries in dictionary\n",
    "def a_dict_to_p_dict(dict, dimvec, prob_mat_0, prob_mat_1, key_0, key_1, ss_n_m):\n",
    "    dict_new = {}\n",
    "    count = 0\n",
    "    for key in dict:\n",
    "        if key_0 in key:\n",
    "            dict_new[key] = L_class_tool(dimvec, prob_mat_0, len(dict[key]), ss_n_m)\n",
    "            count += 1\n",
    "        else:\n",
    "            dict_new[key] = L_class_tool(dimvec, prob_mat_1, len(dict[key]), ss_n_m)\n",
    "            count += 1\n",
    "    \n",
    "    return dict_new   \n",
    "\n",
    "# Expands probability matrices to have the dimensions of the adjacency matrices: makes\n",
    "# [[0.3, 0.2], [0.2, 0.3]] into a 200 by 200 where the top left 100by100 are all 0.3, etc\n",
    "def L_class_tool(dim_vec, p_matrix, dim_mat, SS_n):\n",
    "    test_mat = sbm(n=dim_vec, p=p_matrix)\n",
    "    val_0 = p_matrix[0][0]\n",
    "    val_1 = p_matrix[0][1]\n",
    "    val_2 = p_matrix[1][0]\n",
    "    val_3 = p_matrix[1][1]\n",
    "    for i in range(dim_mat):\n",
    "        for j in range(dim_mat):\n",
    "            if i < SS_n and j < SS_n:\n",
    "                test_mat[i][j] = val_0\n",
    "            elif i < SS_n and j > SS_n:\n",
    "                test_mat[i][j] = val_1\n",
    "            elif i > SS_n and j < SS_n:\n",
    "                test_mat[i][j] = val_2\n",
    "            else :\n",
    "                test_mat[i][j] = val_3\n",
    "    return test_mat\n",
    "\n",
    "def classifier_method(num_graphs, num_graphs_0, num_graphs_1, P_hat_0, P_hat_1, vec, m_n_0, ss_m_n, A_mat):\n",
    "    \n",
    "    pi_coef_hat_0 = num_graphs_0 / num_graphs\n",
    "    pi_coef_hat_1 = num_graphs_1 / num_graphs\n",
    "    \n",
    "    P_0 = L_class_tool(vec, P_hat_0, m_n_0, ss_m_n)\n",
    "    P_1 = L_class_tool(vec, P_hat_1, m_n_0, ss_m_n)\n",
    "    \n",
    "    count = 1\n",
    "    count_1 = 1\n",
    "    for i in ss_m_n:\n",
    "        for j in ss_m_n:\n",
    "            prod = ((P_0[i][j])**(A_mat[i][j]))*((1 - P_0[i][j])**(1 - A_mat[i][j]))\n",
    "            count = count * prod\n",
    "            prod_1 = ((P_1[i][j])**(A_mat[i][j]))*((1 - P_1[i][j])**(1 - A_mat[i][j]))\n",
    "            count_1 = count_1 * prod_1\n",
    "    \n",
    "    class_0 = pi_coef_hat_0 * count\n",
    "    class_1 = pi_coef_hat_1 * count_1\n",
    "    list_val = []\n",
    "    list_val.append(class_0)\n",
    "    list_val.append(class_1)\n",
    "    return np.asarray(list_val)\n",
    "\n",
    "def iterative_screen(a_dict, mat_n_m, y_labels, c, delta, opt):\n",
    "\n",
    "    '''\n",
    "    Performs iterative screening on graphs.\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "    \n",
    "    a_dict: the dictionary of adjacency matrices are going to be used for signal subgraph.\n",
    "\n",
    "    mat_n_m: Length/width of the adjacency matrices in the dictionary\n",
    "    \n",
    "    y_labels: the vector of labels.\n",
    "    \n",
    "    c: the correlation threshold value.\n",
    "    \n",
    "    delta: the quantile threshold value\n",
    "    \n",
    "    opt: indicator of whether to use dcorr or mgc\n",
    "\n",
    "    Returns\n",
    "\n",
    "    -------\n",
    "\n",
    "    values_flags: the signal subgraph of a_matrix, found through iterative\n",
    "\n",
    "    vertex screening.\n",
    "    '''\n",
    "     mgc = MGC()\n",
    "    dcorr = DCorr(which_test = 'unbiased')\n",
    "    S_hat = np.zeros((mat_n_m,1))\n",
    "    count = len(a_dict)\n",
    "    bound = int(count / 2)\n",
    "    for i in range(mat_n_m):\n",
    "      mat_hat = np.zeros((count,mat_n_m))\n",
    "      for j in range(count):\n",
    "        if j % 2 == 0:\n",
    "          mat = a_dict[\"male{0}\".format(j)]\n",
    "        else:\n",
    "          mat = a_dict[\"female{0}\".format(j)]\n",
    "        mat_hat[j] = mat[i]\n",
    "      if opt == \"mgc\":\n",
    "        c_u, independence_test_metadata = mgc.test_statistic(mat_hat, y_labels)\n",
    "        S_hat[i][0] = c_u\n",
    "      else:\n",
    "        c_u, independence_test_metadata = dcorr.test_statistic(mat_hat, y_labels)\n",
    "        S_hat[i][0] = c_u\n",
    "    \n",
    "    S_hat = np.absolute(S_hat)\n",
    "    #as_vec = np.sort(S_hat.reshape(1,200)).reshape(200,1)\n",
    "    #list_comp = as_vec[-20:]\n",
    "    #values_flags = []\n",
    "    #for i in range(20):\n",
    "    #    values_flags.append(np.where(S_hat == list_comp[i])[0][0])\n",
    "    print(S_hat)\n",
    "    values_flags = np.nonzero(S_hat > c)    \n",
    "    return values_flags\n",
    "    \n",
    "def vertex_diff(a_matrix_1, a_matrix_2):\n",
    "    SS = []\n",
    "    for i in range(len(a_matrix_1)):\n",
    "        count = 0\n",
    "        while count < len(a_matrix_1):\n",
    "            if a_matrix_1[i][count] != a_matrix_2[i][count]:\n",
    "                SS.append(i)\n",
    "                count = len(a_matrix_1) + 1\n",
    "            else:\n",
    "                count += 1\n",
    "    SS.sort()\n",
    "    return SS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
